{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu788uHogf75",
        "outputId": "e8063679-f339-443d-9232-11bc0e3ed850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "4FjCO5r0gk8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "VOCAB_SIZE=5000\n",
        "EMBED_DIM=256\n",
        "NUM_HEADS=8\n",
        "NUM_LAYERS=6\n",
        "MAX_SEQ_LEN=128\n",
        "FF_DIM=1024\n",
        "DROPOUT=0.1"
      ],
      "metadata": {
        "id": "Gnemb5xlg3u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Self-Attention\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.qkv = nn.Linear(EMBED_DIM, EMBED_DIM * 3)\n",
        "        self.out = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
        "        self.head_dim = EMBED_DIM // NUM_HEADS\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, NUM_HEADS, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, NUM_HEADS, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, NUM_HEADS, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        mask = torch.tril(torch.ones(T, T)).to(x.device)\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = attn @ v\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out(out)"
      ],
      "metadata": {
        "id": "5akzMGOphaSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer-Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.attn = SelfAttention()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(EMBED_DIM, FF_DIM),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(FF_DIM, EMBED_DIM)\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(EMBED_DIM)\n",
        "        self.ln2 = nn.LayerNorm(EMBED_DIM)\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attn(self.ln1(x)))\n",
        "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "6P4imkwVikEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT Style LLM\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
        "        self.pos_emb = nn.Embedding(MAX_SEQ_LEN, EMBED_DIM)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock() for _ in range(NUM_LAYERS)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(EMBED_DIM)\n",
        "        self.head = nn.Linear(EMBED_DIM, VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        positions = torch.arange(T, device=idx.device)\n",
        "\n",
        "        x = self.token_emb(idx) + self.pos_emb(positions)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -MAX_SEQ_LEN:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, 1)\n",
        "            idx = torch.cat([idx, next_token], dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "TFBp90a_jRBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training loop\n",
        "model = MiniGPT()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "for step in range(1000):\n",
        "    x = torch.randint(0, VOCAB_SIZE, (32, 64))\n",
        "    y = x.clone()\n",
        "\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(-1, VOCAB_SIZE),\n",
        "        y.view(-1)\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpKh9CRQj3ny",
        "outputId": "c9b4effb-3f95-4a74-af9a-251407df54b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 8.661161422729492\n",
            "Loss: 4.571882247924805\n",
            "Loss: 0.9757809042930603\n",
            "Loss: 0.17330001294612885\n",
            "Loss: 0.07771746814250946\n",
            "Loss: 0.04615394026041031\n",
            "Loss: 0.03115539811551571\n",
            "Loss: 0.023439552634954453\n",
            "Loss: 0.01808004267513752\n",
            "Loss: 0.014514915645122528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-OFhdIRj6C4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}